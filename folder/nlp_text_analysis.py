# -*- coding: utf-8 -*-
"""NLP TEXT ANALYSIS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bGdDC2N-HEBsM1ANWkO5JA2OFrK7E6d5

# **NLP TEXT ANALYSIS**

import libraries
"""

import nltk
from nltk.tokenize import word_tokenize
import os
import rarfile
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import cmudict
import string
from nltk.corpus import stopwords
import os
import re
nltk.download('punkt')
nltk.download('cmudict')
from nltk.corpus import cmudict
import math
import string

pip install rarfile

"""## 1 sentiment analysis

1.1 cleaning using stopwords list
1.2	Creating a dictionary of Positive and Negative words
1.3	Extracting Derived variables
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import os

stop_words = set()
stopwords_dir = '/content/drive/MyDrive/StopWords'  #importing stopwords from given stopwords folder

def load_stopwords_from_directory(directory):  #created afunction load stopwords from directory
    for root, dirs, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)
            with open(file_path, 'r', encoding='latin-1') as f:
                stop_words.update(set(f.read().splitlines()))

load_stopwords_from_directory(stopwords_dir) # Load stopwords from directory
def clean_text(text):
    tokens = word_tokenize(text)
    cleaned_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]
    return cleaned_tokens

positive_words = set(open('/content/positive-words.txt', encoding='utf-8').read().splitlines())  #Creating a dictionary of Positive and Negative words
negative_words = set(open('/content/negative-words.txt', encoding='latin-1').read().splitlines())

def calculate_sentiment_scores(text):   #Extracting Derived variables
    cleaned_tokens = clean_text(text)
    positive_score = sum(1 for word in cleaned_tokens if word in positive_words)  #This line calculates the number of positive words present in the cleaned_tokens list.
    negative_score = -1 * sum(1 for word in cleaned_tokens if word in negative_words) #This line calculates the number of negative words present in the cleaned_tokens list
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001) #it is in range -1 to 1 it measures sentiment of is postive or negative ,0.00001 is added to avoid zero error.
    subjectivity_score = (positive_score + negative_score) / (len(cleaned_tokens) + 0.000001) # this score measures subjectivity or emotional intensity of text.its range is 0 to 1
    return {
        'positive_score': positive_score,
        'negative_score': negative_score,
        'polarity_score': polarity_score,
        'subjectivity_score': subjectivity_score
    }

file_path = r'/content/blackassign0001.txt'    #filepath of single text file
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

scores = calculate_sentiment_scores(text)
print("Positive Score:", scores['positive_score']) # Print sentiment scores
print("Negative Score:", scores['negative_score'])
print("Polarity Score:", scores['polarity_score'])
print("Subjectivity Score:", scores['subjectivity_score'])
#here we can get output of Positive Score,Negative Score,Polarity Score,Subjectivity Score

"""# 2 Analysis of Readability"""

def count_syllables(word):
    d = cmudict.dict()   # using cmu pranouncing dictionary ,contains mappings between words and their phonetic representation
    return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0] if word.lower() in d else 0

def calculate_readability(text): # this Function to calculate readability using Gunning Fox index
    sentences = sent_tokenize(text)
    num_words = len(word_tokenize(text)) # number of words and number of sentences
    num_sentences = len(sentences)

    avg_sentence_length = num_words / num_sentences  #average sentence length
    threshold_syllables = 3
    num_complex_words = sum(1 for word in word_tokenize(text) if count_syllables(word) >= threshold_syllables) # Count complex words

    percentage_complex_words = (num_complex_words / num_words) * 100 # percentage of complex words

    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)  #formula multiplies the sum of the average sentence length and the percentage of complex words by a constant factor of 0.4.it derived readability level

    return {
        'average_sentence_length': avg_sentence_length,
        'percentage_complex_words': percentage_complex_words,
        'fog_index': fog_index
    }

file_path = r'/content/blackassign0001.txt'  #filepath of single text file
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

readability_scores = calculate_readability(text)

print("Average Sentence Length:", readability_scores['average_sentence_length'])  # Print readability scores
print("Percentage of Complex Words:", readability_scores['percentage_complex_words'])
print("Fog Index:", readability_scores['fog_index'])

from google.colab import drive
drive.mount('/content/drive')

"""3	Average Number of Words Per Sentence"""

def calculate_avg_words_per_sentence(text):
    sentences = sent_tokenize(text)  # Tokenize the text into sentences
    total_words = 0
    total_sentences = len(sentences)
    for sentence in sentences:      #each sentence into words and count total words
        words = word_tokenize(sentence)
        total_words += len(words)
    avg_words_per_sentence = total_words / total_sentences  # average number of words per sentence

    return avg_words_per_sentence

file_path = '/content/blackassign0001.txt'    #filepath of single text file
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

avg_words_per_sentence = calculate_avg_words_per_sentence(text)

print("Average Number of Words Per Sentence:", avg_words_per_sentence) #print output of Average Number of Words Per Sentence

"""4 Complex word count"""

def calculate_readability(text):
    sentences = sent_tokenize(text)
    words = [word.lower() for sent in sentences for word in word_tokenize(sent)]
    num_words = len(words)  #number of words and number of sentences
    num_sentences = len(sentences)
    avg_sentence_length = num_words / num_sentences #average sentence length

    complex_word_count = sum(1 for word in words if count_syllables(word) > 2)  # Count complex words (words with more than two syllables)
    percentage_complex_words = (complex_word_count / num_words) * 100  # percentage of complex words

    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words) # Calculate Fog Index
    return {
        'average_sentence_length': avg_sentence_length,
        'percentage_complex_words': percentage_complex_words,
        'fog_index': fog_index,
        'complex_word_count': complex_word_count
    }

    file_path = '/content/blackassign0001.txt'   #filepath of single text file
    with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

    complex_wordcount=calculate_readability(text)
    print('complex_word_count:', complex_wordcount['complex_word_count'])

"""5	Word Count"""

def calculate_readability(text):  ## Function to readability using Gunning Fox index
    sentences = sent_tokenize(text)  # Tokenize sentences
    words = [word.lower() for sent in sentences for word in word_tokenize(sent) if word not in string.punctuation] #    # Tokenize words, remove punctuation, and convert to lowercase
    stop_words = set(stopwords.words('english'))   # Remove stopwords
    words = [word for word in words if word not in stop_words]

    num_words = len(words)    # number of words and number of sentences
    num_sentences = len(sentences)
    avg_sentence_length = num_words / num_sentences  #average sentence length
    complex_word_count = sum(1 for word in words if count_syllables(word) > 2)

    percentage_complex_words = (complex_word_count / num_words) * 100  #percentage of complex words

    # Calculate Fog Index
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    return {
        'average_sentence_length': avg_sentence_length,
        'percentage_complex_words': percentage_complex_words,
        'fog_index': fog_index,
        'complex_word_count': complex_word_count,
        'total_cleaned_word_count': num_words
    }

    file_path = '/content/blackassign0001.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

    total_cleaned_words=calculate_readability(text)
    print('total_cleaned_word_count:',total_cleaned_words['total_cleaned_word_count'])

"""6	Syllable Count Per Word"""

def count_syllables(word):
    # Exceptions: words ending with "es" or "ed" don't count those as syllables
    if word.endswith("es") or word.endswith("ed"):
        word = word[:-2]

    # Count the number of vowels in the word
    vowels = "aeiouAEIOU"
    syllables = 0
    prev_char_was_vowel = False
    for char in word:
        if char in vowels:
            if not prev_char_was_vowel:
                syllables += 1
                prev_char_was_vowel = True
        else:
            prev_char_was_vowel = False

    # Adjust the syllable count for words with no vowels
    if syllables == 0:
        syllables = 1

    return syllables

def count_syllables_per_word(text):
    # Split text into words
    words = text.split()

    # Count syllables for each word
    syllable_counts = {}
    for word in words:
        syllable_counts[word] = count_syllables(word)

    return syllable_counts



file_path = '/content/blackassign0001.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

syllable_counts = count_syllables_per_word(text)
output_list = []
for word, syllables in syllable_counts.items():
    output_list.append((word, syllables))

print(output_list)

def count_syllables(word):
    # Exceptions: words ending with "es" or "ed" don't count those as syllables
    if word.endswith("es") or word.endswith("ed"):
        word = word[:-2]

    # Count the number of vowels in the word
    vowels = "aeiouAEIOU"
    syllables = 0
    prev_char_was_vowel = False
    for char in word:
        if char in vowels:
            if not prev_char_was_vowel:
                syllables += 1
                prev_char_was_vowel = True
        else:
            prev_char_was_vowel = False

    # Adjust the syllable count for words with no vowels
    if syllables == 0:
        syllables = 1

    return syllables

def count_syllables_per_word(text):
    # Split text into words
    words = text.split()

    # Count syllables for each word
    syllable_counts = {}
    for word in words:
        syllable_counts[word] = count_syllables(word)

    return syllable_counts



file_path = '/content/blackassign0001.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

syllable_counts = count_syllables_per_word(text)
for word, syllables in syllable_counts.items():
 print(f"{word}: {syllables}")

"""7	Personal Pronouns"""

import re

def count_personal_pronouns(text):
    # Define the personal pronouns and compile the regex pattern
    pronouns_pattern = re.compile(r'\b(I|we|my|ours|us)\b', re.IGNORECASE)

    # Find all matches of the pronouns in the text
    matches = pronouns_pattern.findall(text)

    # Exclude "US" from the matches
    matches = [match for match in matches if match.lower() != 'us']

    # Count the occurrences of each pronoun
    counts = {}
    for pronoun in ['I', 'we', 'my', 'ours', 'us']:
        counts[pronoun] = matches.count(pronoun.lower())

    return counts

# Example usage:
file_path = '/content/blackassign0001.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

counts = count_personal_pronouns(text)
print(counts)

"""8	Average Word Length"""

import nltk

def average_word_length(text):
    # Tokenize the text into words
    words = nltk.word_tokenize(text)

    # Calculate the total number of characters in all words
    total_characters = sum(len(word) for word in words)

    # Calculate the total number of words
    total_words = len(words)
    if total_words > 0:          # Calculate the average word length
        average_length = total_characters / total_words
    else:
        average_length = 0  # Handle the case when there are no words in the text

    return average_length

# Download NLTK resources (if not already downloaded)
nltk.download('punkt')

# Example usage:
file_path = '/content/blackassign0001.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()
avg_length = average_word_length(text)
print("Average word length:", avg_length)

"""1 Here's the combined code to analyze multiple text files, and calculate various text analysis metrics for each file, and generate scores for each text file. comboine all the codes using different functions for each metrics scores and used a rar file to give 100 text files as input and used respective libraries.

2 The output scores are converted to excel file using pandas
"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
import os
import rarfile
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import cmudict
import string
import re

nltk.download('punkt')
nltk.download('cmudict')

# Step 1: Load stopwords
stop_words = set()
stopwords_dir = '/content/drive/MyDrive/StopWords'

def load_stopwords_from_directory(directory):
    for root, dirs, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)
            with open(file_path, 'r', encoding='latin-1') as f:
                stop_words.update(set(f.read().splitlines()))

load_stopwords_from_directory(stopwords_dir)

# Step 2: Load positive and negative words
positive_words = set(open('/content/positive-words.txt', encoding='utf-8').read().splitlines())
negative_words = set(open("/content/negative-words.txt", encoding='latin-1').read().splitlines())

# Step 3: Define function to clean text and calculate sentiment scores
def clean_text(text):
    tokens = word_tokenize(text)
    cleaned_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]
    return cleaned_tokens

def calculate_sentiment_scores(text):
    cleaned_tokens = clean_text(text)
    positive_score = sum(1 for word in cleaned_tokens if word in positive_words)
    negative_score = -1 * sum(1 for word in cleaned_tokens if word in negative_words)
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score) / (len(cleaned_tokens) + 0.000001)
    return {
        'positive_score': positive_score,
        'negative_score': negative_score,
        'polarity_score': polarity_score,
        'subjectivity_score': subjectivity_score
    }

cmu_dict = cmudict.dict()

# Function to count syllables in a word using CMU Pronouncing Dictionary
def count_syllables(word):
    pronunciation = cmu_dict.get(word.lower())
    if pronunciation:
        return len([ph for ph in pronunciation[0] if ph[-1].isdigit()])
    else:
        return 0
# Function to calculate readability using Gunning Fox index
def readability(text):
    # Tokenize sentences
    sentences = sent_tokenize(text)

    # Tokenize words
    words = [word.lower() for sent in sentences for word in word_tokenize(sent)]

    # Calculate number of words and number of sentences
    num_words = len(words)
    num_sentences = len(sentences)

    # Calculate average sentence length
    avg_sentence_length = num_words / num_sentences

    # Define criteria for identifying complex words
    threshold_syllables = 3

    # Count complex words
    num_complex_words = sum(1 for word in words if count_syllables(word) >= threshold_syllables)

    # Calculate percentage of complex words
    percentage_complex_words = (num_complex_words / num_words) * 100

    # Calculate Fog Index
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    return {
        'average_sentence_length': avg_sentence_length,
        'percentage_complex_words': percentage_complex_words,
        'fog_index': fog_index
    }

def calculate_avg_words_per_sentence(text):
    # Tokenize the text into sentences
    sentences = sent_tokenize(text)

    # Initialize variables to store total number of words and sentences
    total_words = 0
    total_sentences = len(sentences)

    # Tokenize each sentence into words and count total words
    for sentence in sentences:
        words = word_tokenize(sentence)
        total_words += len(words)

    # Calculate average number of words per sentence
    avg_words_per_sentence = total_words / total_sentences

    return avg_words_per_sentence

def calculate_readable(text):
    # Tokenize sentences
    sentences = sent_tokenize(text)

    # Tokenize words
    words = [word.lower() for sent in sentences for word in word_tokenize(sent)]

    # Calculate number of words and number of sentences
    num_words = len(words)
    num_sentences = len(sentences)

    # Calculate average sentence length
    #avg_sentence_length = num_words / num_sentences

    # Count complex words (words with more than two syllables)
    complex_word_count = sum(1 for word in words if count_syllables(word) > 2)

    # Calculate percentage of complex words
    #percentage_complex_words = (complex_word_count / num_words) * 100

    # Calculate Fog Index
   # fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    return {

        'complex_word_count': complex_word_count
    }

nltk.download('stopwords')
nltk.download('punkt')
# Function to calculate readability using Gunning Fox index
def calculate_read(text):
    # Tokenize sentences
    sentences = sent_tokenize(text)

    # Tokenize words, remove punctuation, and convert to lowercase
    words = [word.lower() for sent in sentences for word in word_tokenize(sent) if word not in string.punctuation]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Calculate number of words and number of sentences
    num_words = len(words)
    num_sentences = len(sentences)

    # Calculate average sentence length
    #avg_sentence_length = num_words / num_sentences

    # Count complex words (words with more than two syllables)
    #complex_word_count = sum(1 for word in words if count_syllables(word) > 2)

    # Calculate percentage of complex words
    #percentage_complex_words = (complex_word_count / num_words) * 100

    # Calculate Fog Index
    #fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    return {

        'total_cleaned_word_count': num_words
    }

def average_word_length(text):
    # Tokenize the text into words
    words = nltk.word_tokenize(text)

    # Calculate the total number of characters in all words
    total_characters = sum(len(word) for word in words)

    # Calculate the total number of words
    total_words = len(words)

    # Calculate the average word length
    if total_words > 0:
        average_length = total_characters / total_words
    else:
        average_length = 0  # Handle the case when there are no words in the text

    return average_length

def count_personal_pronouns(text):
    # Define the personal pronouns and compile the regex pattern
    pronouns_pattern = re.compile(r'\b(I|we|my|ours|us)\b', re.IGNORECASE)

    # Find all matches of the pronouns in the text
    matches = pronouns_pattern.findall(text)

    # Exclude "US" from the matches
    matches = [match for match in matches if match.lower() != 'US']

    # Count the occurrences of each pronoun
    counts = {}
    for pronoun in ['I', 'we', 'my', 'ours', 'us']:
        counts[pronoun] = matches.count(pronoun.lower())

    return counts

def count_syllables(word):
    # Exceptions: words ending with "es" or "ed" don't count those as syllables
    if word.endswith("es") or word.endswith("ed"):
        word = word[:-2]

    # Count the number of vowels in the word
    vowels = "aeiouAEIOU"
    syllables = 0
    prev_char_was_vowel = False
    for char in word:
        if char in vowels:
            if not prev_char_was_vowel:
                syllables += 1
                prev_char_was_vowel = True
        else:
            prev_char_was_vowel = False

    # Adjust the syllable count for words with no vowels
    if syllables == 0:
        syllables = 1

    return syllables

def count_syllables_per_word(text):
    # Split text into words
    words = text.split()

    # Count syllables for each word
    syllable_counts = {}
    for word in words:
        syllable_counts[word] = count_syllables(word)

    return syllable_counts

# Step 4: Process multiple text files
rar_path = '/content/extracted_articles (2).rar'

output_path = []
with rarfile.RarFile(rar_path, 'r') as rar:
    text_files = [file_info for file_info in rar.infolist() if not file_info.is_dir() and file_info.filename.endswith('.txt')]


    # Iterate through each text file in the archive
    for file_info in text_files:
        # Extract the file
        file_content = rar.read(file_info)

        # Decode the file content (assuming it's encoded in latin-1)
        text = file_content.decode('latin-1')


        scores = calculate_sentiment_scores(text)        #calling all the functions
        read = readability(text)
        avg_words_per_sentence = calculate_avg_words_per_sentence(text)
        complex_wordcount=calculate_readable(text)
        total_cleaned_words=calculate_read(text)
        avg_length = average_word_length(text)
        counts = count_personal_pronouns(text)
        syllable_counts = count_syllables_per_word(text)
        output_list = []
        for word, syllables in syllable_counts.items():
             output_list.append((word, syllables))


        output_data.append({
            'File': file_info.filename,
            'Positive Score':  scores['positive_score'],           #here created an output data list to convert to excel sheet
            'Negative Score': scores['negative_score'],
            'Polarity Score':scores['polarity_score'],
            'Subjectivity Score': scores['subjectivity_score'],
            'Average Sentence Length': read['average_sentence_length'],
            'Percentage of Complex Words': read['percentage_complex_words'],
            'Fog Index': read['fog_index'],
            'Average Number of Words Per Sentence': avg_words_per_sentence,
            'Complex Word Count': complex_wordcount['complex_word_count'],
            'Total Cleaned Word Count': total_cleaned_words['total_cleaned_word_count'],
            'Average Word Length': avg_length,
            'Personal Pronoun Counts': counts,
            'Syllable Counts': syllable_counts
        })

        # Print scores for the current text file
        print("File:", file_info.filename)
        print("Positive Score:", scores['positive_score'])
        print("Negative Score:", scores['negative_score'])
        print("Polarity Score:", scores['polarity_score'])
        print("Subjectivity Score:", scores['subjectivity_score'])
        # Print readability scores
        print("Average Sentence Length:", read['average_sentence_length'])
        print("Percentage of Complex Words:", read['percentage_complex_words'])
        print("Fog Index:", read['fog_index'])
        print("Average Number of Words Per Sentence:", avg_words_per_sentence)
        print('complex_word_count:', complex_wordcount['complex_word_count'])
        print('total_cleaned_word_count:',total_cleaned_words['total_cleaned_word_count'])
        print(output_list)
        print(counts)
        print("Average word length:", avg_length)

        print("----------------------")


# Create a DataFrame from the output data
df = pd.DataFrame(output_data)

# Export the DataFrame to an Excel file
excel_file_path ='/content/new excel - Copy.xlsx'        #output excel file path
df.to_excel(excel_file_path, index=False)

print("Excel file successfully created at:", excel_file_path)

"""Excel file successfully created at: /content/new excel - Copy.xlsx

# The analysis has been successfully completed, yielding all scores and metrics as per the provided instructions.
"""